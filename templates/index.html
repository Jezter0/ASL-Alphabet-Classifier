{% extends "layout.html" %}
{% block title %}ASL Live Recognition{% endblock %}

{% block content %}

<h1 class="text-center fw-bold mb-4">ASL Live Prediction</h1>

<div class="row justify-content-center">
    <div class="col-md-8">

        <div class="card p-4 shadow-lg border-0 rounded-4">

            <div class="d-flex justify-content-between align-items-center mb-3">
                <h4 class="h3 m-0">Live Camera Feed</h4>

                <!-- Model Selector -->
                <select id="modelSelect" class="form-select w-auto shadow-sm">
                    <option value="cnn">CNN</option>
                    <option value="efficientnet">EfficientNet</option>
                    <option value="convnext">ConvNeXt</option>
                    <option value="resnet">ResNet</option>
                </select>
            </div>

            <div class="position-relative text-center">
                <video id="cam" autoplay playsinline class="rounded shadow-sm border"
                       style="width: 100%; max-height: 420px;"></video>

                <canvas id="overlay" class="position-absolute top-0 start-0"
                        style="pointer-events:none;"></canvas>
            </div>

            <div class="text-center mt-3">
                <button class="btn btn-primary btn-lg px-4 shadow-sm" id="startBtn">Start Camera</button>
            </div>

            <hr class="my-4">

            <h5 class="text-center text-info fw-semibold">Prediction:</h5>
            <h1 id="prediction" class="text-center display-3 text-success fw-bold">—</h1>

        </div>

    </div>
</div>

<script type="module">
import {
    HandLandmarker,
    FilesetResolver
} from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest";

let handLandmarker;
let video = document.getElementById("cam");
let canvas = document.getElementById("overlay");
let ctx = canvas.getContext("2d");
let running = false;

async function initMP() {
    const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm"
    );

    handLandmarker = await HandLandmarker.createFromOptions(vision, {
        baseOptions: {
            modelAssetPath:
                "https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task"
        },
        runningMode: "VIDEO",
        numHands: 1
    });

    console.log("MediaPipe Initialized");
}

async function startCamera() {
    await initMP();

    const stream = await navigator.mediaDevices.getUserMedia({ video: true });
    video.srcObject = stream;

    video.onloadeddata = () => {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        running = true;
        detectFrame();
    };
}

async function detectFrame() {
    if (!running) return;

    const results = handLandmarker.detectForVideo(video, performance.now());
    ctx.clearRect(0, 0, canvas.width, canvas.height);

    if (results.landmarks && results.landmarks.length > 0) {
        const lm = results.landmarks[0];
        drawLandmarks(lm);
        processROI(lm);  // crop + resize + send
    }

    requestAnimationFrame(detectFrame);
}

function drawLandmarks(landmarks) {
    ctx.fillStyle = "lime";
    landmarks.forEach(pt => {
        ctx.beginPath();
        ctx.arc(pt.x * canvas.width, pt.y * canvas.height, 4, 0, 2 * Math.PI);
        ctx.fill();
    });
}

function processROI(landmarks) {
    const w = video.videoWidth;
    const h = video.videoHeight;

    const xs = landmarks.map(l => l.x * w);
    const ys = landmarks.map(l => l.y * h);

    let xmin = Math.min(...xs);
    let xmax = Math.max(...xs);
    let ymin = Math.min(...ys);
    let ymax = Math.max(...ys);

    // padding
    const pad = 40;
    xmin = Math.max(0, xmin - pad);
    ymin = Math.max(0, ymin - pad);
    xmax = Math.min(w, xmax + pad);
    ymax = Math.min(h, ymax + pad);

    drawBBox(xmin, ymin, xmax, ymax);

    cropAndSend(xmin, ymin, xmax, ymax);
}

function drawBBox(xmin, ymin, xmax, ymax) {
    ctx.strokeStyle = "red";
    ctx.lineWidth = 3;
    ctx.strokeRect(xmin, ymin, xmax - xmin, ymax - ymin);
}

function cropAndSend(xmin, ymin, xmax, ymax) {
    const w = xmax - xmin;
    const h = ymax - ymin;

    const cropCanvas = document.createElement("canvas");
    cropCanvas.width = w;
    cropCanvas.height = h;

    const cx = cropCanvas.getContext("2d");
    cx.drawImage(video, xmin, ymin, w, h, 0, 0, w, h);

    prepareForPrediction(cropCanvas);
}

function prepareForPrediction(cropCanvas) {
    const TARGET = 224; // EfficientNet input size

    const resizedCanvas = document.createElement("canvas");
    resizedCanvas.width = TARGET;
    resizedCanvas.height = TARGET;

    const rc = resizedCanvas.getContext("2d");
    rc.drawImage(cropCanvas, 0, 0, TARGET, TARGET);

    // Send as JPEG -> Flask will apply preprocess_input
    resizedCanvas.toBlob(blob => {
        const fd = new FormData();
        fd.append("frame", blob, "frame.jpg");

        fetch("/predict", {
            method: "POST",
            body: fd
        })
        .then(res => res.json())
        .then(data => {
            document.getElementById("prediction").textContent =
                data.prediction || "—";
        });
    }, "image/jpeg", 0.8);
}

document.getElementById("startBtn").onclick = startCamera;
</script>

{% endblock %}